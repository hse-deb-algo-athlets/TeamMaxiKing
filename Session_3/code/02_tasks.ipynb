{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Simple vector embedding generation\n",
    "\n",
    "**Objective:**\n",
    "Generate vector embeddings from text data.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load huggingface embedding model (`model_name=\"sentence-transformers/all-mpnet-base-v2\"`)\n",
    "- embed simple text queries\n",
    "\n",
    "How to select the right embedding model: [MTEB - Massive Text Embedding Benchmark](https://huggingface.co/blog/mteb)\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Chroma](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/TeamMaxiKing/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/workspaces/TeamMaxiKing/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector length: 768\n",
      "[-0.048951808363199234, -0.039862047880887985, -0.021562788635492325, 0.009908556006848812, -0.03810393065214157, 0.012684349901974201, 0.04349448159337044, 0.07183390110731125, 0.009748606011271477, -0.006987082306295633]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Perform vector search\n",
    "query_vector = embedding_model.embed_query(text=text)\n",
    "\n",
    "print(f\"Embedding vector length: {len(query_vector)}\")\n",
    "print(query_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Generate embedding vectors with custom dataset\n",
    "\n",
    "**Objective:**\n",
    "Load custom dataset, preprocess it and generate vector embeddings.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load pdf document \"AI_Book.pdf\" via langchain document loader: `PyPDFLoader`\n",
    "- use RecursiveCharacterTextSplitter to split documents into chunks\n",
    "- generate embeddings for single documents\n",
    "\n",
    "**RecursiveCharacterTextSplitter:**\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain PyPDFLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)\n",
    "- [Langchain RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Aurélien GéronHands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent SystemsSECOND EDITION\n",
      "Boston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing' metadata={'source': './AI_Book.pdf', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "pdf_doc = \"./AI_Book.pdf\"\n",
    "\n",
    "# Create pdf data loader\n",
    "# ADD HERE YOUR CODE\n",
    "loader = PyPDFLoader(file_path=pdf_doc)\n",
    "\n",
    "# Load and split documents in chunks\n",
    "# ADD HERE YOUR CODE\n",
    "pages = loader.load()\n",
    "pages_chunked = RecursiveCharacterTextSplitter().split_documents(pages)\n",
    "\n",
    "# Function to clean text by removing invalid unicode characters, including surrogate pairs\n",
    "def clean_document_text(chunk):\n",
    "    # Remove surrogate pairs\n",
    "    text = chunk.page_content\n",
    "    text = re.sub(r'[\\ud800-\\udfff]', '', text)\n",
    "    # Optionally remove non-ASCII characters (depends on your use case)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return Document(page_content=text, metadata=chunk.metadata)\n",
    "\n",
    "pages_chunked_cleaned = [clean_document_text(chunk) for chunk in pages_chunked]\n",
    "\n",
    "print(pages_chunked_cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pages_chunked_cleaned[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of text chunks: {len(pages_chunked_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Store vector embeddings from pdf document to ChromaDB vector database.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Store vector embeddings into ChromaDB to store knowledge.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- create chromadb client\n",
    "- create chromadb collection\n",
    "- create langchain chroma db client\n",
    "- store text document chunks and vector embeddings to vector databases\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain How To](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#initialization-from-client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"localhost\",\n",
    "    port=8000,\n",
    "    ssl=False,\n",
    "    headers=None,\n",
    "    settings=Settings(allow_reset=True, anonymized_telemetry=False),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Create a collection\n",
    "\n",
    "collection_name = \"AI_Book\"\n",
    "\n",
    "collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Create chromadb\n",
    "vector_db_from_client = Chroma(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['47ebb199-ac7a-4fe3-b3ae-a094756c8ea6',\n",
       " '61786197-c7d3-466e-8e40-9aa3a1134f1d',\n",
       " '0e550d96-f29a-4fcd-bd60-067d4c9e29a1',\n",
       " '386c718e-56a0-4860-85d7-f5d28a5c289f',\n",
       " '6c57dd8e-da9f-4f99-827f-a96cbca375d9',\n",
       " 'ecae7bb7-ebd0-4b94-9d44-7e5c12ef3bd6',\n",
       " '0573a319-8eb7-4345-8a12-a19e90a5cf9b',\n",
       " '38d3ee78-9f9b-4510-8f44-f16e7602e62d',\n",
       " '950a321b-c4e7-4e4b-9b2a-5667765322c8',\n",
       " 'edfb6e6e-9b90-4fb8-b13f-c361f15ae864',\n",
       " 'f107498a-68db-4509-8ed0-5a97b36e9646',\n",
       " 'b0835cde-6de9-42cf-850f-9231c87880bc',\n",
       " '199eda86-353c-444e-ab93-d0e685005433',\n",
       " '4ef73735-4dc7-4c63-b992-2e6dbcd3415d',\n",
       " '8b5e0347-1ac5-46b6-8eb7-0a9737027367',\n",
       " '87aca02e-d65b-4c4f-9fe1-ebc67489ec22',\n",
       " 'fce61e03-40bf-4782-aa5d-a821b6dc0bf9',\n",
       " '0e05208a-d17f-4aa9-939b-1b254b0e1693',\n",
       " 'e7e2a7b9-ee5c-4573-81fa-a5651e7d955d',\n",
       " 'ccac0e9c-7903-4ae4-80fb-b26793a35e00',\n",
       " 'e05fd145-7fa9-4ab3-9c51-24d1f9691bcf',\n",
       " 'a8e16440-1152-4981-a1c7-45752bbe4f59',\n",
       " '821e0f88-a7d3-4418-a451-473ac03e31d6',\n",
       " '8695ac46-7ab2-4bf7-b7e0-cb2751f4f3d3',\n",
       " '626fae68-db48-49b9-9f10-286ed0e96eb2',\n",
       " 'a5177751-ff94-4032-a62f-2c01e8a176cc',\n",
       " 'd6053e0f-3fc7-456c-8e7b-f79115d89b58',\n",
       " 'f771d122-a01c-4895-8191-90ccc64b4bdf',\n",
       " 'a1e96601-d196-4408-b5d3-c11c08fe77ad',\n",
       " '3f862d5d-5ee5-4435-92bb-de61a6e07c99',\n",
       " 'cc41205a-03c7-4884-be65-36b3e4d29aaa',\n",
       " '2fccd96d-c696-42e4-8316-2df64014932e',\n",
       " '5bd873f9-1240-41a5-8a81-a1a3fdbfd468',\n",
       " '90328936-5e84-46f6-b102-e198a673f720',\n",
       " 'ce380b0f-9627-41c4-b33b-589105204a58',\n",
       " '99965ff3-3152-4149-b8b5-e2ec0e3ca777',\n",
       " 'ba8f48ba-e1e7-444a-9339-c29f46cf2829',\n",
       " 'a2f363a2-b931-467f-b3e3-d8a28372adda',\n",
       " '2b53fc5d-a66b-4b1f-b6e3-174f0322961d',\n",
       " '9f515033-7ebd-4745-a68e-aeb2458b5aea',\n",
       " '3113d107-c41b-49b9-a1e4-561e8fa38199',\n",
       " '1719baca-4b0e-4f47-b169-20aae6d81635',\n",
       " '7b5ab899-3c8a-4c5b-b469-f6e8238b9c3b',\n",
       " '40b35c7a-5543-4b35-a2b8-64b40e41341f',\n",
       " 'cc3b019f-53e5-46fc-9ae7-c6eec949047f',\n",
       " 'c2d93531-32d5-4e7a-bdc1-43e34de14d41',\n",
       " 'afb24e61-01d8-43d3-8c0b-95f943c069a7',\n",
       " 'cb74fe7c-3cc9-47a9-a0eb-20baa4d87126',\n",
       " '2f12e9fe-cec0-41f9-a497-b5bb3318e491',\n",
       " '279b034f-c6bb-4027-850f-da4a2b6b3066']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(pages_chunked_cleaned[:50]))]\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "vector_db_from_client.add_documents(documents=pages_chunked_cleaned[:50], id=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_collection(\"ai_model_book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Access ChromaDB and perform vector search\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Use query to perform vector search against ChromaDB vector database\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- define query\n",
    "- run vector search\n",
    "- print k=3 most similar documents\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Query ChromaDB](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#query-directly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of Machine Learning Systems\n",
      "There are so many different types of Machine Learning systems that it is useful to\n",
      "classify them in broad categories based on:\n",
      "Whether or not they are trained with human supervision (supervised, unsuper\n",
      "vised, semisupervised, and Reinforcement Learning)\n",
      "Whether or not they can learn incrementally on the fly (online versus batch\n",
      "learning)\n",
      "Whether they work by simply comparing new data points to known data points,\n",
      "or instead detect patterns in the training data and build a predictive model, much\n",
      "like scientists do (instance-based versus model-based learning)\n",
      "These criteria are not exclusive; you can combine them in any way you like. For\n",
      "example, a state-of-the-art spam filter may learn on the fly using a deep neural net\n",
      "work model trained using examples of spam and ham; this makes it an online, model-\n",
      "based, supervised learning system.\n",
      "Lets look at each of these criteria a bit more closely.\n",
      "Supervised/Unsupervised Learning\n",
      "Machine Learning systems can be classified according to the amount and type of\n",
      "supervision they get during training. There are four major categories: supervised\n",
      "learning, unsupervised learning, semisupervised learning, and Reinforcement Learn\n",
      "ing.\n",
      "Supervised learning\n",
      "In supervised learning , the training data you feed to the algorithm includes the desired\n",
      "solutions, called labels  (Figure 1-5 ).\n",
      "Figure 1-5. A labeled training set for supervised learning (e.g., spam classification)\n",
      "8 | Chapter 1: The Machine Learning Landscape\n",
      "{'page': 33, 'source': './AI_Book.pdf'}\n",
      "\n",
      "----------------\n",
      "\n",
      "Types of Machine Learning Systems\n",
      "There are so many different types of Machine Learning systems that it is useful to\n",
      "classify them in broad categories based on:\n",
      "Whether or not they are trained with human supervision (supervised, unsuper\n",
      "vised, semisupervised, and Reinforcement Learning)\n",
      "Whether or not they can learn incrementally on the fly (online versus batch\n",
      "learning)\n",
      "Whether they work by simply comparing new data points to known data points,\n",
      "or instead detect patterns in the training data and build a predictive model, much\n",
      "like scientists do (instance-based versus model-based learning)\n",
      "These criteria are not exclusive; you can combine them in any way you like. For\n",
      "example, a state-of-the-art spam filter may learn on the fly using a deep neural net\n",
      "work model trained using examples of spam and ham; this makes it an online, model-\n",
      "based, supervised learning system.\n",
      "Lets look at each of these criteria a bit more closely.\n",
      "Supervised/Unsupervised Learning\n",
      "Machine Learning systems can be classified according to the amount and type of\n",
      "supervision they get during training. There are four major categories: supervised\n",
      "learning, unsupervised learning, semisupervised learning, and Reinforcement Learn\n",
      "ing.\n",
      "Supervised learning\n",
      "In supervised learning , the training data you feed to the algorithm includes the desired\n",
      "solutions, called labels  (Figure 1-5 ).\n",
      "Figure 1-5. A labeled training set for supervised learning (e.g., spam classification)\n",
      "8 | Chapter 1: The Machine Learning Landscape\n",
      "{'page': 33, 'source': './AI_Book.pdf'}\n",
      "\n",
      "----------------\n",
      "\n",
      "Figure 1-12. Reinforcement Learning\n",
      "For example, many robots implement Reinforcement Learning algorithms to learn\n",
      "how to walk. DeepMinds AlphaGo program is also a good example of Reinforcement\n",
      "Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie\n",
      "at the game of Go. It learned its winning policy by analyzing millions of games, and\n",
      "then playing many games against itself. Note that learning was turned off during the\n",
      "games against the champion; AlphaGo was just applying the policy it had learned.\n",
      "Batch and Online Learning\n",
      "Another criterion used to classify Machine Learning systems is whether or not the\n",
      "system can learn incrementally from a stream of incoming data.\n",
      "Batch learning\n",
      "In batch learning , the system is incapable of learning incrementally: it must be trained\n",
      "using all the available data. This will generally take a lot of time and computing\n",
      "resources, so it is typically done offline. First the system is trained, and then it is\n",
      "launched into production and runs without learning anymore; it just applies what it\n",
      "has learned. This is called offline  learning .\n",
      "If you want a batch learning system to know about new data (such as a new type of\n",
      "spam), you need to train a new version of the system from scratch on the full dataset\n",
      "(not just the new data, but also the old data), then stop the old system and replace it\n",
      "with the new one.\n",
      "Fortunately, the whole process of training, evaluating, and launching a Machine\n",
      "Learning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\n",
      "Types of Machine Learning Systems | 15\n",
      "{'page': 40, 'source': './AI_Book.pdf'}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Types of Machine Learning Systems\"\n",
    "\n",
    "results = vector_db_from_client.similarity_search(\n",
    "    search_query,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    print(res.page_content)\n",
    "    print(res.metadata)\n",
    "    print(\"\\n----------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
