{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Simple vector embedding generation\n",
    "\n",
    "**Objective:**\n",
    "Generate vector embeddings from text data.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load huggingface embedding model (`model_name=\"sentence-transformers/all-mpnet-base-v2\"`)\n",
    "- embed simple text queries\n",
    "\n",
    "How to select the right embedding model: [MTEB - Massive Text Embedding Benchmark](https://huggingface.co/blog/mteb)\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Chroma](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/TeamMaxiKing/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/workspaces/TeamMaxiKing/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "embedding_model = HuggingFaceEmbeddings (model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector length: 768\n",
      "[-0.04895179718732834, -0.03986203297972679, -0.021562788635492325, 0.00990856159478426, -0.038103923201560974, 0.01268438808619976, 0.04349448159337044, 0.07183388620615005, 0.009748609736561775, -0.006987091153860092]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Perform vector search\n",
    "query_vector = embedding_model.embed_query(text)\n",
    "\n",
    "print(f\"Embedding vector length: {len(query_vector)}\")\n",
    "print(query_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Generate embedding vectors with custom dataset\n",
    "\n",
    "**Objective:**\n",
    "Load custom dataset, preprocess it and generate vector embeddings.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load pdf document \"AI_Book.pdf\" via langchain document loader: `PyPDFLoader`\n",
    "- use RecursiveCharacterTextSplitter to split documents into chunks\n",
    "- generate embeddings for single documents\n",
    "\n",
    "**RecursiveCharacterTextSplitter:**\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain PyPDFLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)\n",
    "- [Langchain RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Aurélien GéronHands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent SystemsSECOND EDITION\n",
      "Boston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing' metadata={'source': './AI_Book.pdf', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "pdf_doc = \"./AI_Book.pdf\"\n",
    "\n",
    "# Create pdf data loader\n",
    "# ADD HERE YOUR CODE\n",
    "loader = PyPDFLoader(pdf_doc)\n",
    "\n",
    "# Load and split documents in chunks\n",
    "# ADD HERE YOUR CODE\n",
    "pages_chunked = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # Specified splitting characters\n",
    "    chunk_size=1000,  # Adjust to your desired chunk size\n",
    "    chunk_overlap=100  # Overlap between chunks, useful for context\n",
    ")\n",
    "pages = loader.load()  # Load all pages from the PDF\n",
    "pages_chunked = text_splitter.split_documents(pages)\n",
    "\n",
    "# Function to clean text by removing invalid unicode characters, including surrogate pairs\n",
    "def clean_text(text):\n",
    "    # Remove surrogate pairs\n",
    "    text = re.sub(r'[\\ud800-\\udfff]', '', text)\n",
    "    # Optionally remove non-ASCII characters (depends on your use case)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return text\n",
    "\n",
    "pages_chunked_cleaned = [clean_text(chunk.page_content) for chunk in pages_chunked]\n",
    "\n",
    "print(pages_chunked[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='978-1-492-03264-9\n",
      "[LSI]Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
      "by Aurélien Géron\n",
      "Copyright © 2019 Aurélien Géron. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\n",
      "sales department: 800-998-9938 or corporate@oreilly.com .\n",
      "Editor:  Nicole Tache\n",
      "Interior Designer:  David FutatoCover Designer:  Karen Montgomery\n",
      "Illustrator:  Rebecca Demarest\n",
      "June 2019:  Second Edition\n",
      "Revision History for the Early Release\n",
      "2018-11-05: First Release\n",
      "2019-01-24: Second Release\n",
      "2019-03-07: Third Release\n",
      "2019-03-29: Fourth Release\n",
      "2019-04-22: Fifth Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781492032649  for release details.' metadata={'source': './AI_Book.pdf', 'page': 3}\n"
     ]
    }
   ],
   "source": [
    "print(pages_chunked[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 1272\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of text chunks: {len(pages_chunked)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Store vector embeddings from pdf document to ChromaDB vector database.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Store vector embeddings into ChromaDB to store knowledge.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- create chromadb client\n",
    "- create chromadb collection\n",
    "- create langchain chroma db client\n",
    "- store text document chunks and vector embeddings to vector databases\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain How To](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#initialization-from-client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"localhost\",\n",
    "    port=8000,\n",
    "    ssl=False,\n",
    "    headers=None,\n",
    "    settings=Settings(allow_reset=True, anonymized_telemetry=False),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Create a collection\n",
    "\n",
    "collection_name = \"AI_Book\"\n",
    "\n",
    "collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Create chromadb\n",
    "vector_db_from_client = Chroma(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['88e5df45-3147-4e74-8557-7066f16fb1b6',\n",
       " 'bf7baa9d-7834-40a8-9a15-c7edcc150c87',\n",
       " '214533c9-6bac-4280-b7a4-4f407de1a133',\n",
       " '57fbe9f5-aa60-4f11-9f08-82e04edda58b',\n",
       " '1a0797f0-7034-42b0-abd7-905aefc0df23',\n",
       " '8a00f77f-ee45-483b-907e-ffda154cfa33',\n",
       " '9990c6ce-0e17-4702-8c35-f393c35b2d0e',\n",
       " 'e7937e69-a93e-461d-a56f-e4f146b0dd69',\n",
       " '4d5ee7de-20f9-47e4-9cf5-e1ee85d8f350',\n",
       " '1c1d2066-e7b4-4c69-9782-d177fad2eff3',\n",
       " '7776410c-acf2-4218-ae9b-26e44fee4ed7',\n",
       " '97409fec-4eda-4cbd-8103-8d5f2d9a3ca7',\n",
       " '63a0f92d-0661-47df-b59c-e5c5c117b8bf',\n",
       " '67f3b3a7-86d6-4ac5-87a7-02ea6b397741',\n",
       " '7f22ec34-ae53-4493-9f26-9412a39d9ee9',\n",
       " 'f73cf84a-c4fc-4f87-a886-b2055d230176',\n",
       " 'e01d0f2f-b327-496b-8599-ebf940e73c66',\n",
       " '28d9c2ee-6598-48bb-a657-79634f1d0278',\n",
       " '435eadd3-698b-49d1-86f8-20709bea5c80',\n",
       " '8e009126-ea01-4703-82cc-865047ac6c55',\n",
       " '2e5e1f06-0db8-4027-a34c-ab6ede34b30a',\n",
       " '81bdc0e6-9783-4a3c-9090-a5bd95ee9293',\n",
       " 'c6bc8e22-74c3-4089-878a-5d21e39be0b0',\n",
       " '32b56682-a6b6-49e2-b2f7-f147dcbec131',\n",
       " 'e13525f2-f701-4c33-b77f-fa9d535ae7ca',\n",
       " '78dfd192-3de3-478f-a1e2-28d151616e55',\n",
       " 'c3b4f25a-aafe-4a14-af6d-c06fbc070f28',\n",
       " 'a9606f94-bd16-403e-8df7-2b9019347cea',\n",
       " 'ae7b2c6b-63c0-46c7-aca4-eb1d5f294423',\n",
       " 'bae8eeee-9fbc-44e2-8a5b-aa940c169324',\n",
       " 'c1b32028-c0cb-47d2-a463-acef5c2b54eb',\n",
       " '8a2b653f-0475-4d5f-bc60-9eb6ed1c735f',\n",
       " '7d153739-0b92-41f8-aa51-613aa98af65a',\n",
       " 'ca210a5c-640f-4ddb-b92b-00c03720aae4',\n",
       " '1b452fc3-a3a4-433b-bc64-15e0c6dd813f',\n",
       " 'bc9d4600-f744-45dc-bab9-e0592258e5f6',\n",
       " 'ff31356a-1c2f-4571-803f-8bc01d9d3a38',\n",
       " 'c0ca6508-6fc8-4ec7-aaa9-9c54c077d449',\n",
       " '615b7bef-b398-48c0-9da2-e25b740486ca',\n",
       " '15e61431-f034-435a-9f79-a0ae4016c1da',\n",
       " 'accf70b5-e7c1-4ddc-bccf-98878549c363',\n",
       " 'be7d275e-39c9-42cf-946c-db2acb969146',\n",
       " 'ca17c319-b5b5-4e05-af49-2e939fb564c7',\n",
       " '73dccf1d-553e-49b1-bc13-4bf5e28a3df7',\n",
       " '8051b854-05e1-49d7-bc1c-6d679dc1fb7c',\n",
       " '500a673f-f576-4018-b94c-f2f8fd02058b',\n",
       " 'f6f5f364-c05b-4d59-85ed-39b9ee557845',\n",
       " '18cb8030-1cb1-4b97-a0ab-4b8146e3856e',\n",
       " '0903edcb-14b4-4cae-8804-0f54e8918c47',\n",
       " 'cd6644e5-1862-4b24-966b-4fe748a3414d']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(pages_chunked_cleaned[:50]))]\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "vector_db_from_client.add_texts(texts=pages_chunked_cleaned[:50], ids=uuids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_collection(\"ai_model_book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Access ChromaDB and perform vector search\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Use query to perform vector search against ChromaDB vector database\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- define query\n",
    "- run vector search\n",
    "- print k=3 most similar documents\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Query ChromaDB](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#query-directly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common learning algorithms: Linear and Polynomial Regression,\n",
      "Logistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision\n",
      "Trees, Random Forests, and Ensemble methods.\n",
      "xiv | Preface\n",
      "{}\n",
      "\n",
      "\n",
      "The most common learning algorithms: Linear and Polynomial Regression,\n",
      "Logistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision\n",
      "Trees, Random Forests, and Ensemble methods.\n",
      "xiv | Preface\n",
      "{}\n",
      "\n",
      "\n",
      "The most common learning algorithms: Linear and Polynomial Regression,\n",
      "Logistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision\n",
      "Trees, Random Forests, and Ensemble methods.\n",
      "xiv | Preface\n",
      "{}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Types of Machine Learning Systems\"\n",
    "\n",
    "results = vector_db_from_client.similarity_search(query=search_query, k=3)\n",
    "\n",
    "for res in results:\n",
    "    print(res.page_content)\n",
    "    print(res.metadata)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
