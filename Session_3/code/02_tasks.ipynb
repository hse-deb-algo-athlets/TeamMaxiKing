{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Simple vector embedding generation\n",
    "\n",
    "**Objective:**\n",
    "Generate vector embeddings from text data.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load huggingface embedding model (`model_name=\"sentence-transformers/all-mpnet-base-v2\"`)\n",
    "- embed simple text queries\n",
    "\n",
    "How to select the right embedding model: [MTEB - Massive Text Embedding Benchmark](https://huggingface.co/blog/mteb)\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Chroma](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/TeamMaxiKing/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/workspaces/TeamMaxiKing/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector length: 768\n",
      "[-0.04895174503326416, -0.039861928671598434, -0.021562742069363594, 0.009908485226333141, -0.03810397908091545, 0.012684328481554985, 0.04349445924162865, 0.07183387875556946, 0.00974861066788435, -0.006987082771956921]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Perform vector search\n",
    "query_vector = embedding_model.embed_query(text=text)\n",
    "\n",
    "print(f\"Embedding vector length: {len(query_vector)}\")\n",
    "print(query_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Generate embedding vectors with custom dataset\n",
    "\n",
    "**Objective:**\n",
    "Load custom dataset, preprocess it and generate vector embeddings.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- load pdf document \"AI_Book.pdf\" via langchain document loader: `PyPDFLoader`\n",
    "- use RecursiveCharacterTextSplitter to split documents into chunks\n",
    "- generate embeddings for single documents\n",
    "\n",
    "**RecursiveCharacterTextSplitter:**\n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain PyPDFLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)\n",
    "- [Langchain RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Aurlien GronHands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent SystemsSECOND EDITION\n",
      "Boston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing' metadata={'source': './AI_Book.pdf', 'page': 2}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "pdf_doc = \"./AI_Book.pdf\"\n",
    "\n",
    "# Create pdf data loader\n",
    "# ADD HERE YOUR CODE\n",
    "loader = PyPDFLoader(file_path=pdf_doc)\n",
    "\n",
    "# Load and split documents in chunks\n",
    "# ADD HERE YOUR CODE\n",
    "pages = loader.load()\n",
    "pages_chunked = RecursiveCharacterTextSplitter().split_documents(pages)\n",
    "\n",
    "# Function to clean text by removing invalid unicode characters, including surrogate pairs\n",
    "def clean_document_text(chunk):\n",
    "    # Remove surrogate pairs\n",
    "    text = chunk.page_content\n",
    "    text = re.sub(r'[\\ud800-\\udfff]', '', text)\n",
    "    # Optionally remove non-ASCII characters (depends on your use case)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    return Document(page_content=text, metadata=chunk.metadata)\n",
    "\n",
    "pages_chunked_cleaned = [clean_document_text(chunk) for chunk in pages_chunked]\n",
    "\n",
    "print(pages_chunked_cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='978-1-492-03264-9\n",
      "[LSI]Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
      "by Aurlien Gron\n",
      "Copyright  2019 Aurlien Gron. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by OReilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "OReilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\n",
      "sales department: 800-998-9938 or corporate@oreilly.com .\n",
      "Editor:  Nicole Tache\n",
      "Interior Designer:  David FutatoCover Designer:  Karen Montgomery\n",
      "Illustrator:  Rebecca Demarest\n",
      "June 2019:  Second Edition\n",
      "Revision History for the Early Release\n",
      "2018-11-05: First Release\n",
      "2019-01-24: Second Release\n",
      "2019-03-07: Third Release\n",
      "2019-03-29: Fourth Release\n",
      "2019-04-22: Fifth Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781492032649  for release details.\n",
      "The OReilly logo is a registered trademark of OReilly Media, Inc. Hands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and TensorFlow , the cover image, and related trade dress are trademarks of OReilly\n",
      "Media, Inc.\n",
      "While the publisher and the author have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk. If any code samples or other technology this work contains or describes is subject to open source\n",
      "licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\n",
      "thereof complies with such licenses and/or rights.' metadata={'source': './AI_Book.pdf', 'page': 3}\n"
     ]
    }
   ],
   "source": [
    "print(pages_chunked_cleaned[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 507\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of text chunks: {len(pages_chunked_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Store vector embeddings from pdf document to ChromaDB vector database.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Store vector embeddings into ChromaDB to store knowledge.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- create chromadb client\n",
    "- create chromadb collection\n",
    "- create langchain chroma db client\n",
    "- store text document chunks and vector embeddings to vector databases\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain How To](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#initialization-from-client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"localhost\",\n",
    "    port=8000,\n",
    "    ssl=False,\n",
    "    headers=None,\n",
    "    settings=Settings(allow_reset=True, anonymized_telemetry=False),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,\n",
    ")\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Create a collection\n",
    "\n",
    "collection_name = \"AI_Book\"\n",
    "\n",
    "collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Create chromadb\n",
    "vector_db_from_client = Chroma(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7bad263a-799d-4237-913c-7dc1cce5395c',\n",
       " 'ab39e5e6-caff-4b5f-93c7-ce06ef6676e0',\n",
       " 'ed186336-a30c-4ea6-bd04-6ec59eb21e5a',\n",
       " '7534869e-c1cc-475c-a4d3-f0526a88d15a',\n",
       " 'd2809161-695f-4a7d-9c14-b550a78565a7',\n",
       " 'bdbc2b98-e171-4f9a-b10f-fd24ef5433a4',\n",
       " 'a658c07f-7e98-4326-8c05-52af4049190a',\n",
       " 'd8f4a41c-b0f0-45e2-949e-683a74ef31a1',\n",
       " '6f7263b4-3df2-42b7-8715-a45f49ee4c8e',\n",
       " 'bda205f1-3dec-499d-ba3d-06b328bb21a5',\n",
       " '07da4f71-cbe3-4e89-9c1c-47bdb51bae0d',\n",
       " '73626f70-d4c3-47de-92e2-1308dd18b81c',\n",
       " 'e7fc4348-66d0-4e97-862b-d92ed52e076d',\n",
       " 'c8395b70-dd1e-4264-8bb7-1dcade049608',\n",
       " 'cc5f59f6-59e0-4846-9ba6-e75bec5a892c',\n",
       " 'ca4cab84-2432-4c0c-8b4f-c23d6eeb2a83',\n",
       " '61e48c22-1785-48e6-9375-a4e03230069e',\n",
       " '3fd77d92-fc93-4371-adf6-c0c0372b5c08',\n",
       " 'c6acbb20-901d-405e-a14a-8ed8d6fa6e53',\n",
       " 'ed3ba7cf-2d88-4003-a183-1d24bd2aa780',\n",
       " '4f5811af-c154-4385-a158-cdd0a09e8078',\n",
       " '6af598e7-8db4-4806-85cb-de9ea74d3ec2',\n",
       " '48b91962-25b5-4fe2-9cd3-e30d234817c7',\n",
       " 'e789e041-a58c-4be8-8645-ef7e96df43d2',\n",
       " 'ab29a4c0-68e4-4c60-bd4c-73a6f7a9e45c',\n",
       " 'e125790e-8a00-4daf-bcc1-5e36228a707c',\n",
       " '025a72d7-60c0-4c8c-9a81-ef6945017a48',\n",
       " '7becc36d-392d-4804-ad79-33a4cd9a302a',\n",
       " '95514276-2c15-4edf-9568-8d28a3dfd6ff',\n",
       " '598ba14b-053d-4510-9aa9-a536ffa149a6',\n",
       " '468e49e0-237d-4fbb-b3b0-69cec91d3a33',\n",
       " '9f703e86-6808-4bd2-8c6d-9e7b6f92f548',\n",
       " '90fae2b3-180b-43fd-ac29-667b97427868',\n",
       " 'd6134f48-ee38-40eb-8d7c-1cf65b7d96bc',\n",
       " '24b1c93d-25c3-416e-8754-9dcaf57b7966',\n",
       " '69e308bd-5233-485f-9a6f-4a582c3eeb51',\n",
       " '977d47c5-e9a1-452d-a6e0-41a2b4f42a97',\n",
       " '1a505c60-18f5-4a0b-820e-4ff452a7a222',\n",
       " '510a9b7a-103d-4881-854e-344dae51fe51',\n",
       " '7efab46e-1cf1-43de-8fd8-2d7063f52c2b',\n",
       " 'c13e3589-bc44-47ec-a0f8-84bc0ca89eaa',\n",
       " '07a824ee-07bb-4fa9-9538-5a9e590c58ad',\n",
       " 'd2417ddf-6dd2-46e7-85ca-3980afc76444',\n",
       " '5f3c8156-77b3-454e-9878-dcf7950292d8',\n",
       " 'bc0fc3c2-4e1e-4f09-b7df-5ebfc9b2d9f8',\n",
       " '9002b0dd-92fa-487c-9a88-61448f46e4ca',\n",
       " '4234b77d-48a5-42d5-a36a-5230f0b7cec7',\n",
       " 'b14a59dd-f7c6-48d9-8cf4-7c9b2362612f',\n",
       " '804a5906-703d-4fe0-839b-f6026fb3c1f6',\n",
       " '7a516941-feba-4cdd-a609-4e6f039199db']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(pages_chunked_cleaned[:50]))]\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "vector_db_from_client.add_documents(documents=pages_chunked_cleaned[:50], id=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_collection(\"ai_model_book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Access ChromaDB and perform vector search\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Use query to perform vector search against ChromaDB vector database\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- define query\n",
    "- run vector search\n",
    "- print k=3 most similar documents\n",
    "\n",
    "**Useful links:**\n",
    "\n",
    "- [Langchain Query ChromaDB](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#query-directly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of Machine Learning Systems\n",
      "There are so many different types of Machine Learning systems that it is useful to\n",
      "classify them in broad categories based on:\n",
      "Whether or not they are trained with human supervision (supervised, unsuper\n",
      "vised, semisupervised, and Reinforcement Learning)\n",
      "Whether or not they can learn incrementally on the fly (online versus batch\n",
      "learning)\n",
      "Whether they work by simply comparing new data points to known data points,\n",
      "or instead detect patterns in the training data and build a predictive model, much\n",
      "like scientists do (instance-based versus model-based learning)\n",
      "These criteria are not exclusive; you can combine them in any way you like. For\n",
      "example, a state-of-the-art spam filter may learn on the fly using a deep neural net\n",
      "work model trained using examples of spam and ham; this makes it an online, model-\n",
      "based, supervised learning system.\n",
      "Lets look at each of these criteria a bit more closely.\n",
      "Supervised/Unsupervised Learning\n",
      "Machine Learning systems can be classified according to the amount and type of\n",
      "supervision they get during training. There are four major categories: supervised\n",
      "learning, unsupervised learning, semisupervised learning, and Reinforcement Learn\n",
      "ing.\n",
      "Supervised learning\n",
      "In supervised learning , the training data you feed to the algorithm includes the desired\n",
      "solutions, called labels  (Figure 1-5 ).\n",
      "Figure 1-5. A labeled training set for supervised learning (e.g., spam classification)\n",
      "8 | Chapter 1: The Machine Learning Landscape\n",
      "{'page': 33, 'source': './AI_Book.pdf'}\n",
      "\n",
      "----------------\n",
      "\n",
      "Figure 1-12. Reinforcement Learning\n",
      "For example, many robots implement Reinforcement Learning algorithms to learn\n",
      "how to walk. DeepMinds AlphaGo program is also a good example of Reinforcement\n",
      "Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie\n",
      "at the game of Go. It learned its winning policy by analyzing millions of games, and\n",
      "then playing many games against itself. Note that learning was turned off during the\n",
      "games against the champion; AlphaGo was just applying the policy it had learned.\n",
      "Batch and Online Learning\n",
      "Another criterion used to classify Machine Learning systems is whether or not the\n",
      "system can learn incrementally from a stream of incoming data.\n",
      "Batch learning\n",
      "In batch learning , the system is incapable of learning incrementally: it must be trained\n",
      "using all the available data. This will generally take a lot of time and computing\n",
      "resources, so it is typically done offline. First the system is trained, and then it is\n",
      "launched into production and runs without learning anymore; it just applies what it\n",
      "has learned. This is called offline  learning .\n",
      "If you want a batch learning system to know about new data (such as a new type of\n",
      "spam), you need to train a new version of the system from scratch on the full dataset\n",
      "(not just the new data, but also the old data), then stop the old system and replace it\n",
      "with the new one.\n",
      "Fortunately, the whole process of training, evaluating, and launching a Machine\n",
      "Learning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\n",
      "Types of Machine Learning Systems | 15\n",
      "{'page': 40, 'source': './AI_Book.pdf'}\n",
      "\n",
      "----------------\n",
      "\n",
      "Figure 1-15. Instance-based learning\n",
      "Model-based learning\n",
      "Another way to generalize from a set of examples is to build a model of these exam\n",
      "ples, then use that model to make predictions . This is called model-based learning\n",
      "(Figure 1-16 ).\n",
      "Figure 1-16. Model-based learning\n",
      "For example, suppose you want to know if money makes people happy, so you down\n",
      "load the Better Life Index  data from the OECDs website  as well as stats about GDP\n",
      "per capita from the IMFs website . Then you join the tables and sort by GDP per cap\n",
      "ita. Table 1-1  shows an excerpt of what you get.\n",
      "Types of Machine Learning Systems | 19\n",
      "{'page': 44, 'source': './AI_Book.pdf'}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Types of Machine Learning Systems\"\n",
    "\n",
    "results = vector_db_from_client.similarity_search(\n",
    "    search_query,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    print(res.page_content)\n",
    "    print(res.metadata)\n",
    "    print(\"\\n----------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
