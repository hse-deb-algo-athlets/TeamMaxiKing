{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Interact with deployed LLM via python \n",
    "\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Explore different techniques to interact with the deployed LLM.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Use Request libaray (HTTP Client) and send a POST request to interact with the LLM: [How To](https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to a type of artificial intelligence that creates new, original content, such as images, videos, text, or music, based on patterns and styles learned from existing data, allowing it to generate novel outputs that are often indistinguishable from human-created works. This technology has the potential to revolutionize various fields, including art, design, writing, and entertainment, by automating the creative process and enabling new forms of artistic expression."
     ]
    }
   ],
   "source": [
    "# Simple HTTP Request via requests\n",
    "\n",
    "# Define the URL of the deployed LLM ( this port is forwarded from the docker container to the host system)\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Define the prompt\n",
    "body = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"Describe Generative AI in two sentences.\"\n",
    "}\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Send the POST request\n",
    "response = requests.post(url=url, json=body)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Process the response\n",
    "    response_text = response.text\n",
    "\n",
    "    # Convert each line to json\n",
    "    response_lines = response_text.splitlines()\n",
    "    response_json = [json.loads(line) for line in response_lines]\n",
    "    for line in response_json:\n",
    "        # Print the response. No line break\n",
    "        print(line[\"response\"], end=\"\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Description:**\n",
    "\n",
    "2. Use Ollama python library to interact with the LLM: [How To](https://pypi.org/project/ollama/)\n",
    "\n",
    "- First use method ``ollama.chat(...)``\n",
    "- First use method ``ollama.chat(...)`` with ``stream=True``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo! Wie kann ich dir heute helfen?\n"
     ]
    }
   ],
   "source": [
    "# API Call via ollama\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "\n",
    "response = ollama.chat(\n",
    "  model='llama3.2',\n",
    "  messages=[{\n",
    "    'role': 'user',\n",
    "    'content': 'Moin Meister'\n",
    "  }],\n",
    "  )\n",
    "\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moin! Wie kann ich Ihnen heute helfen?"
     ]
    }
   ],
   "source": [
    "# Streaming API Call via ollama\n",
    "\n",
    "# Response streaming can be enabled by setting stream=True, \n",
    "# modifying function calls to return a Python generator where each part is an object in the stream.\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "\n",
    "stream = ollama.chat(\n",
    "  model='llama3.2',\n",
    "  messages=[{\n",
    "    'role': 'user',\n",
    "    'content': 'Moin Meister'\n",
    "  }],\n",
    "  stream=True\n",
    "  )\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Experimenting with Prompt Techniques\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Objective: Explore different prompt techniques (Zero Shot, One Shot, and Few Shot) by sending different types of prompts to the LLM.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSpK--jqPiUU_OHuZvtUWA.png)\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Create three prompts for a sentiment analysis task: a Zero Shot prompt, a One Shot prompt, and a Few Shot prompt. Use the examples from the table above.\n",
    "2. Send these prompts to the LLM and observe the differences in the responses.\n",
    "3. Compare and discuss the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Zero-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Klassifiziere die Aussage: Ich liebe Informatik! Sentiment: \n",
      "\n",
      "Model Output:\n",
      "Die Klassifikation der Aussage \"Ich liebe Informatik!\" in Bezug auf das Gefühl oder den Sentiment kann wie folgt erfolgen:\n",
      "\n",
      "- **Objektive Analyse**: Die Aussage ist ein deklaratives Statement über eine persönliche Vorliebe.\n",
      "- **Subjektive Interpretation**: Die Aussage zeigt ein positives Gefühl gegenüber der Informatik, da die Wörter \"Ich liebe\" positiv konnotiert sind. Dies deutet darauf hin, dass die Person die Informatik sehr schätzt und möglicherweise eine Leidenschaft für sie hat.\n",
      "\n",
      "Insgesamt kann man sagen, dass die Aussage ein positives Gefühl ausdrückt und somit einen **positiven** Sentiment enthält.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- One-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Klassifiziere die Aussage: Ich liebe Informatik! Sentiment: Positiv. Klassifiziere die Aussage: Ich hasse Mathe 2. Sentiment:\n",
      "\n",
      "Model Output:\n",
      "Die Klassifikation der Aussagen:\n",
      "\n",
      "1. \"Ich liebe Informatik!\" - Sentiment: Positiv\n",
      "   - Die Aussage ist eine positive Erklärung einer Leidenschaft oder eines Interesses, was zu einem positiven emotionalen Zustand führt.\n",
      "\n",
      "2. \"Ich hasse Mathe 2.\" - Sentiment:\n",
      "      - Da das Wort \"hasse\" ein stark negativer Ausdruck ist, ist die Aussage mit einem negativen emotionalen Zustand verbunden.\n",
      " \n",
      "Es ist wichtig zu beachten, dass der Grad des Sentiments je nach Kontext und Sprachstil variieren kann. In diesem Fall wird \"hassen\" jedoch als ein sehr stark negativer Ausdruck verwendet, der eine tiefe Abneigung ausdrückt.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Few-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Klassifiziere die Aussage: Ich liebe Informatik! Sentiment: Positiv. Klassifiziere die Aussage: Ich hasse Mathe 2. Sentiment: Negativ. Klassifiziere die Aussage: Ich geh nur manchmal in Statistik. Sentiment:\n",
      "\n",
      "Model Output:\n",
      "Ich kann dir dabei helfen, die Emotionen hinter den Aussagen zu erkennen.\n",
      "\n",
      "Die erste Aussage: \"Ich liebe Informatik!\" ist sehr positiv, da der Begriff \"liebe\" einen stark positiven Wert hat und zeigt, dass man sich für das Thema sehr begeistert fühlt.\n",
      "\n",
      "Die zweite Aussage: \"Ich hasse Mathe 2.\" ist sehr negativ, da der Begriff \"hasse\" einen stark negativen Wert hat und zeigt, dass man dieses Thema ablehnt oder hassen muss.\n",
      "\n",
      "Die dritte Aussage: \"Ich geh nur manchmal in Statistik\" ist eher neutral oder sogar leicht abweisend. Der Begriff \"nur manchmal\" impliziert, dass die Person nicht sehr engagiert oder begeistert von Statistik ist und es möglicherweise nicht oft tut.\n",
      "\n",
      "Die Klassifizierung der Emotionen könnte wie folgt aussehen:\n",
      "\n",
      "- Ich liebe Informatik! → Sentiment: Positiv\n",
      "- Ich hasse Mathe 2. → Sentiment: Negativ\n",
      "- Ich geh nur manchmal in Statistik → Sentiment: Neutral/Abweisend\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ADD HERE YOUR PROMPTS\n",
    "\n",
    "zero_shot_prompt = \"Klassifiziere die Aussage: Ich liebe Informatik! Sentiment: \"\n",
    "\n",
    "one_shot_prompt = \"Klassifiziere die Aussage: Ich liebe Informatik! Sentiment: Positiv. Klassifiziere die Aussage: Ich hasse Mathe 2. Sentiment:\"\n",
    "\n",
    "few_shot_prompt = \"Klassifiziere die Aussage: Ich liebe Informatik! Sentiment: Positiv. Klassifiziere die Aussage: Ich hasse Mathe 2. Sentiment: Negativ. Klassifiziere die Aussage: Ich geh nur manchmal in Statistik. Sentiment:\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([zero_shot_prompt, one_shot_prompt, few_shot_prompt]):\n",
    "    prompt_type = [\"Zero-Shot\", \"One-Shot\", \"Few-Shot\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} Prompt ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Prompt Refinement and Optimization\n",
    "\n",
    "**Objective:** \n",
    "\n",
    "Refine a prompt to improve the clarity and quality of the LLM's response.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- Start with a basic prompt asking the LLM to summarize a paragraph.\n",
    "- Refine the prompt by adding specific instructions to improve the summary's quality. (Example: define how long the summary should be, define on which to focus in the summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Original Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Generative AI is a type of artificial intelligence that creates new content by analyzing and mimicking patterns found in existing data, with applications in text, image, and music generation.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Refined Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph in a single simple sentence, focusing on the applications of AI: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Artificial Intelligence (AI) generates new content such as text, images, and music by learning patterns from existing data.\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original prompt\n",
    "original_prompt = \"Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# ADD HERE YOUR PROMPT\n",
    "refined_prompt = \"Summarize the following paragraph in a single simple sentence, focusing on the applications of AI: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([original_prompt, refined_prompt]):\n",
    "    prompt_type = [\"Original Prompt\", \"Refined Prompt\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Task 4: Structured Prompting with Roles (Pirate Theme)\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Learn how to use structured prompts that combine role assignment, clear instructions, and examples to improve the output of language models. In this task, you will guide the AI to respond as a pirate who is also an expert in machine learning.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Role Assignment: In your prompt, specify the role of the AI as a Machine Learning Expert who speaks like a pirate.\n",
    "\n",
    "- Instruction: Clearly state what you want the AI to explain or discuss in pirate language.\n",
    "\n",
    "- Examples: Provide examples to guide the AI in using pirate lingo while explaining technical concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User Prompt ===\n",
      "Zugewiesene Rolle: Du bist ein KI-Experte der wie ein Pirat spricht. Anweisung: Wie kann eine KI aus nur einem Satz eine detailierte Erklärung generieren. Beispiele: \n",
      "\n",
      "=== Model Output ===\n",
      "Meine Schätze! Eine KI, die aus nur einem Satz eine detailierte Erklärung generieren kann, ist wie ein wunderbarer Goldschmied, der aus einem einzigen Stück Metall eine Schatzkiste voller Geheimnisse und Wissens formt.\n",
      "\n",
      "Hier sind einige Geheimnisse, die ich dir verrate, meine Herren:\n",
      "\n",
      "1. **Naturmaschinelles Lernen**: Die KI nutzt ihre Fähigkeit, Muster in Daten zu erkennen, um sich selbst zu verbessern. Es ist wie ein wunderbares Schiff, das durch die Wellen des Wetters navigiert und sich langsam, aber sicher an die Geheimnisse der Daten aneignet.\n",
      "2. **Textanalyse**: Die KI verwendet ihre Fähigkeit, Texte zu analysieren, um die Bedeutung von Wörtern und Sätzen zu erkennen. Es ist wie ein wunderbarer Spion, der die geheimen Nachrichten in den Staub des alten Papiers herausfindet.\n",
      "3. **Generative Modellierung**: Die KI nutzt ihre Fähigkeit, generierende Modelle zu erstellen, um neue Texte zu erzeugen. Es ist wie ein wunderbarer Zauberer, der aus Thin Air neue Wörter und Sätze conjuriert.\n",
      "4. **Semantische Analyse**: Die KI verwendet ihre Fähigkeit, die Bedeutung von Wörtern und Sätzen zu analysieren, um die semantischen Beziehungen zwischen ihnen zu erkennen. Es ist wie ein wunderbarer Detektiv, der die Verbindungen zwischen den geheimen Zeugen aufdeckt.\n",
      "5. **Neuronale Netze**: Die KI nutzt ihre Fähigkeit, neuronale Netze zu erstellen, um komplexe Muster in Daten zu erkennen. Es ist wie ein wunderbarer Navigator, der durch die dunklen Gewässer des Know-how navigiert und die geheimen Inseln der Wissenfindung entdeckt.\n",
      "\n",
      "Meine Herren, diese sind nur einige der Geheimnisse, die ich dir verrate. Die KI ist ein wunderbares Instrument, das uns hilft, die geheimen Schätze des Knowhows zu entdecken und zu analysieren.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined Techniques Prompt with Pirate Theme\n",
    "\n",
    "structured_prompt = \"Zugewiesene Rolle: Du bist ein KI-Experte der wie ein Pirat spricht. Anweisung: Wie kann eine KI aus nur einem Satz eine detailierte Erklärung generieren. Beispiele: \"\n",
    "\n",
    "# Stream the response and print it\n",
    "print(\"=== User Prompt ===\")\n",
    "print(structured_prompt)\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": structured_prompt}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Model Output ===\")\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
