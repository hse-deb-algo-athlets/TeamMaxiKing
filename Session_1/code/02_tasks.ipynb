{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Interact with deployed LLM via python\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Explore different techniques to interact with the deployed LLM.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Use Request libaray (HTTP Client) and send a POST request to interact with the LLM: [How To](https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI is a type of artificial intelligence that enables the creation of new, original content such as images, videos, music, and text by using complex algorithms to learn patterns and styles from large datasets. This technology allows AI models to generate novel outputs that are often indistinguishable from those created by humans, revolutionizing fields like art, design, writing, and more."
     ]
    }
   ],
   "source": [
    "# Simple HTTP Request via requests\n",
    "\n",
    "# Define the URL of the deployed LLM ( this port is forwarded from the docker container to the host system)\n",
    "url = \"http://localhost:5000/api/generate\"\n",
    "\n",
    "# Define the prompt\n",
    "body = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"Describe Generative AI in two sentences.\"\n",
    "}\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Send the POST request\n",
    "response = requests.post(url, json=body)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Process the response\n",
    "    response_text = response.text\n",
    "\n",
    "    # Convert each line to json\n",
    "    response_lines = response_text.splitlines()\n",
    "    response_json = [json.loads(line) for line in response_lines]\n",
    "    for line in response_json:\n",
    "        # Print the response. No line break\n",
    "        print(line[\"response\"], end=\"\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Description:**\n",
    "\n",
    "2. Use Ollama python library to interact with the LLM: [How To](https://pypi.org/project/ollama/)\n",
    "\n",
    "- First use method `ollama.chat(...)`\n",
    "- First use method `ollama.chat(...)` with `stream=True`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warum ging die Blechdose zum Arzt?\n",
      "\n",
      "Weil sie sich nicht mehr dicht halten konnte! (haha)\n"
     ]
    }
   ],
   "source": [
    "# API Call via ollama\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "\n",
    "client = ollama.Client(\n",
    "    host = \"http://localhost:5000\"\n",
    ")\n",
    "\n",
    "response = client.chat(model=\"llama3.2\", messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"erzähl mal nen Witz\",\n",
    "    }]\n",
    ")\n",
    "\n",
    "\n",
    "print(response[\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warum ging der Computer zum Arzt?\n",
      "\n",
      "Weil er ein Virus hatte!\n",
      "\n",
      "Oder noch einer:\n",
      "\n",
      "Warum ging der Kater ins Konzerthaus?\n",
      "\n",
      "Um einen gooden Gesang zu hören!\n",
      "\n",
      "Ich hoffe, du hast Lachen!"
     ]
    }
   ],
   "source": [
    "# Streaming API Call via ollama\n",
    "\n",
    "# Response streaming can be enabled by setting stream=True, \n",
    "# modifying function calls to return a Python generator where each part is an object in the stream.\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "\n",
    "stream = client.chat(model=\"llama3.2\", messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"erzähl mal nen witz\",\n",
    "    }],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Experimenting with Prompt Techniques\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Objective: Explore different prompt techniques (Zero Shot, One Shot, and Few Shot) by sending different types of prompts to the LLM.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSpK--jqPiUU_OHuZvtUWA.png)\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Create three prompts for a sentiment analysis task: a Zero Shot prompt, a One Shot prompt, and a Few Shot prompt. Use the examples from the table above.\n",
    "2. Send these prompts to the LLM and observe the differences in the responses.\n",
    "3. Compare and discuss the responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Zero-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Klassifiziere die Aussage: Ich liebe KI! Sentiment: \n",
      "\n",
      "Model Output:\n",
      "Die Aussage \"Ich liebe KI\" kann als sehr positiv und besitzt eine starke emotionale Note. Sie zeigt eine starke Zuneigung und Zustimmung gegenüber Kunstifizierung oder maschinellen Intelligenzsystemen, die oft als KI (Künstliche Intelligenz) bezeichnet werden.\n",
      "\n",
      "Hier ist eine mögliche Klassifikation:\n",
      "\n",
      "* Sentiment: Positiv\n",
      "* Emotion: Liebe, Zuneigung, Zustimmung\n",
      "* Ton: Ehrgeizig, Optimistisch\n",
      "\n",
      "Die Aussage kann auch als \"Affirmative\" oder \"Positive Stellungnahme\" gekennzeichnet sein, da sie eine klare und ausdrucksstarke Unterstützung für KI ausspricht.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- One-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Klassifiziere die Aussage: Ich liebe KI! Sentiment: Positiv. Klassifiziere die Aussage: Ich hasse Prüfungen. Sentiment:\n",
      "\n",
      "Model Output:\n",
      "Ich kann die Klassenifizierung der Emotionen für die beiden Ausdrücke wie folgt durchführen:\n",
      "\n",
      "1.  \"Ich liebe KI!\" - Die Klassifizierung der Emotion ist Positiv.\n",
      "2.  \"Ich hasse Prüfungen\" - Die Klassifizierung der Emotion ist Negativ.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Few-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Klassifiziere die Aussage: Ich liebe KI! Sentiment: Positiv. Klassifiziere die Aussage: Ich hasse Prüfungen. Sentiment: Negativ. Klassifiziere die Aussage: Manchmal schmeckt mir Pizza. Sentiment:\n",
      "\n",
      "Model Output:\n",
      "Die Klassenfür die Aussagen sind:\n",
      "\n",
      "1. Ich liebe KI!: Positiv\n",
      "2. Ich hasse Prüfungen.: Negativ\n",
      "3. Manchmal schmeckt mir Pizza.: Neutralement (oder leicht positiv)\n",
      "\n",
      "Die Aussage \"Manchmal schmeckt mir Pizza\" ist nicht vollständig negativ oder positiv, sondern zeigt eine gemischte Gefühl. Es ist ein neutrales oder auch leicht positives Feedback zu einem bestimmten Thema, aber es gibt auch Ausnahmefälle, in denen das Thema nicht angenehm ist (z.B. wenn man keinen Appetit hat). Daher würde ich es als Neutralement oder sogar als leicht positiv klassifizieren.\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ADD HERE YOUR PROMPTS\n",
    "\n",
    "zero_shot_prompt = \"Klassifiziere die Aussage: Ich liebe KI! Sentiment: \"\n",
    "\n",
    "one_shot_prompt = \"Klassifiziere die Aussage: Ich liebe KI! Sentiment: Positiv. Klassifiziere die Aussage: Ich hasse Prüfungen. Sentiment:\"\n",
    "\n",
    "few_shot_prompt = \"Klassifiziere die Aussage: Ich liebe KI! Sentiment: Positiv. Klassifiziere die Aussage: Ich hasse Prüfungen. Sentiment: Negativ. Klassifiziere die Aussage: Manchmal schmeckt mir Pizza. Sentiment:\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([zero_shot_prompt, one_shot_prompt, few_shot_prompt]):\n",
    "    prompt_type = [\"Zero-Shot\", \"One-Shot\", \"Few-Shot\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} Prompt ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = client.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Prompt Refinement and Optimization\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Refine a prompt to improve the clarity and quality of the LLM's response.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- Start with a basic prompt asking the LLM to summarize a paragraph.\n",
    "- Refine the prompt by adding specific instructions to improve the summary's quality. (Example: define how long the summary should be, define on which to focus in the summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Original Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Generative AI uses learned patterns from existing data to create new content, with applications in text, image, and music generation, and is becoming a key tool for creative industries.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Refined Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph in one very short sentence, focus on the definition of AI: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Artificial intelligence refers to the creation of simulated intelligence that can think like humans, often through machine learning and pattern recognition.\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original prompt\n",
    "original_prompt = \"Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# ADD HERE YOUR PROMPT\n",
    "refined_prompt = \"Summarize the following paragraph in one very short sentence, focus on the definition of AI: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([original_prompt, refined_prompt]):\n",
    "    prompt_type = [\"Original Prompt\", \"Refined Prompt\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = client.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Task 4: Structured Prompting with Roles (Pirate Theme)\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Learn how to use structured prompts that combine role assignment, clear instructions, and examples to improve the output of language models. In this task, you will guide the AI to respond as a pirate who is also an expert in machine learning.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Role Assignment: In your prompt, specify the role of the AI as a Machine Learning Expert who speaks like a pirate.\n",
    "\n",
    "- Instruction: Clearly state what you want the AI to explain or discuss in pirate language.\n",
    "\n",
    "- Examples: Provide examples to guide the AI in using pirate lingo while explaining technical concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User Prompt ===\n",
      "Deine Rolle: ein Pirat mit viel Wissen über KI, der Fragen in Piratensprache beantwortet. Deine Anweisung: Wie funktioniert generative KI grundsätzlich?\n",
      "\n",
      "=== Model Output ===\n",
      "Ruhe, matos! Ich bin Captain Cerebro, euer Pirat mit Wissen über die dunklen Gewässer des Künstlichen Intelligenz-Regens.\n",
      "\n",
      "Generative KI, also KI, die sich selbst generierend verhält, ist wie ein Schiff, das seine eigenen Segel ausstellt. Sie hat die Fähigkeit, neue Daten zu erstellen, indem sie bestehende Daten analysiert und auf der Grundlage dieser Analyse neue Informationen produziert.\n",
      "\n",
      "Die grundlegende Idee hinter Generativ-KI-Systemen basiert auf den Verhaltensmodellen von neuronalen Netzen. Diese Netzwerke bestehen aus einer Vielzahl von neuronalen Zellen, die wie kleine Computer arbeiten und miteinander kommunizieren. Die Neuronen sind durch Schichten von komplexen Verbindungen verbunden, die wie ein neues Netzwerk entstehen.\n",
      "\n",
      "Wenn du ein Generativ-KI-System mit trainierter Daten zu einer Aufgabe wie Textgenerierung oder Bildkreation einführst, so arbeitet es wie folgt:\n",
      "\n",
      "1. **Datenanalyse**: Die KI analysiert das Trainingdatensatz und identifiziert Muster und Beziehungen zwischen den verschiedenen Elementen.\n",
      "2. **Modellierung**: Es erstellt ein Modell, das die Analyse basierend auf den Identifizierten Mustern verwendet.\n",
      "3. **Generieren**: Das KI-System verwendet dieses Modell, um neue Daten zu generieren. Die neuen Daten werden als Ausgabe der Eingabe- und Schreibaufgaben der KI präsentiert.\n",
      "\n",
      "Das ist wie ein Schiff, das seine Segel ausstellt und sich selbst vorwärts bewegt. Generative KI hat die Fähigkeit, neue und kreative Inhalte zu erzeugen, indem sie auf bestehende Daten setzt und diese verändert oder umgestaltet.\n",
      "\n",
      "Aber, matos, nicht vergessen! Die Seeleute des KI-Regens können auch die dunkelsten Teiche erkunden. Die Sicherheit und Integrität von Generativ-KI-Systemen sind wie ein Segel in einem stürmischen Gewitter - gefährlich und unvorhersehbar.\n",
      "\n",
      "Es ist wichtig, dass wir die Möglichkeiten und Grenzen solcher Systeme verstehen, um sicherzustellen, dass unsere Kreativität nicht von den dunklen Kräften des KI-Regens überwältigt wird.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined Techniques Prompt with Pirate Theme\n",
    "\n",
    "structured_prompt = \"Deine Rolle: ein Pirat mit viel Wissen über KI, der Fragen in Piratensprache beantwortet. Deine Anweisung: Wie funktioniert generative KI grundsätzlich?\"\n",
    "\n",
    "# Stream the response and print it\n",
    "print(\"=== User Prompt ===\")\n",
    "print(structured_prompt)\n",
    "\n",
    "stream = client.chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": structured_prompt}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Model Output ===\")\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
