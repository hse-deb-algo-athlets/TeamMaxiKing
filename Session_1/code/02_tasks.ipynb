{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Interact with deployed LLM via python \n",
    "\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Explore different techniques to interact with the deployed LLM.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Use Request libaray (HTTP Client) and send a POST request to interact with the LLM: [How To](https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to a type of artificial intelligence that can create original content, such as text, images, or music, by generating new variations of existing models or patterns, often through complex algorithms and training data. By combining human creativity with computational power, generative AI can produce highly realistic and engaging outputs that can be used in various applications, including art, design, and entertainment."
     ]
    }
   ],
   "source": [
    "# Simple HTTP Request via requests\n",
    "\n",
    "# Define the URL of the deployed LLM ( this port is forwarded from the docker container to the host system)\n",
    "url = \"http://localhost:11434/api/generate/\"\n",
    "\n",
    "# Define the prompt\n",
    "body = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"Describe Generative AI in two sentences.\"\n",
    "}\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# Send the POST request\n",
    "response = requests.post(url=url,json=body)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Process the response\n",
    "    response_text = response.text\n",
    "\n",
    "    # Convert each line to json\n",
    "    response_lines = response_text.splitlines()\n",
    "    response_json = [json.loads(line) for line in response_lines]\n",
    "    for line in response_json:\n",
    "        # Print the response. No line break\n",
    "        print(line[\"response\"], end=\"\")\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Description:**\n",
    "\n",
    "2. Use Ollama python library to interact with the LLM: [How To](https://pypi.org/project/ollama/)\n",
    "\n",
    "- First use method ``ollama.chat(...)``\n",
    "- First use method ``ollama.chat(...)`` with ``stream=True``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue to us because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh. He discovered that when sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen and oxygen.\n",
      "\n",
      "These molecules scatter the light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths. This is because the smaller molecules are more effective at scattering the blue light.\n",
      "\n",
      "As a result, the blue light is distributed throughout the atmosphere, giving the sky its blue color. The other colors of the visible spectrum, like red and orange, are scattered less and appear more intense near the horizon due to longer wavelengths.\n",
      "\n",
      "It's worth noting that this effect is more pronounced during the daytime when the sun is overhead, and it's why the sky often appears more brilliant and vibrant during this time. However, at sunrise and sunset, the light has to travel through more of the atmosphere, which scatters the shorter wavelengths, making the colors appear more muted and red.\n",
      "\n",
      "Overall, the combination of Rayleigh scattering and the Earth's atmospheric conditions creates the beautiful blue color we see in the sky every day.\n"
     ]
    }
   ],
   "source": [
    "# API Call via ollama\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "\n",
    "response = ollama.chat(model=model, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue to us because of a phenomenon called Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described it in the late 19th century.\n",
      "\n",
      "When sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen (N2) and oxygen (O2). These gas molecules are much smaller than the wavelength of light, so they scatter the light in all directions. However, they scatter shorter wavelengths of light, like blue and violet, more than longer wavelengths, like red and orange.\n",
      "\n",
      "This is because the smaller molecules are more effective at scattering the shorter wavelengths, which means that the blue and violet light is dispersed throughout the atmosphere, giving the sky its characteristic blue color. The amount of scattering depends on the altitude and density of the gas molecules, but in the lower atmosphere, Rayleigh scattering dominates.\n",
      "\n",
      "It's worth noting that during sunrise and sunset, the light has to travel through more of the Earth's atmosphere to reach our eyes, which means it encounters a larger number of gas molecules. This is why the sky often takes on an orange or red hue during these times.\n",
      "\n",
      "Overall, the blue color of the sky is a result of the way sunlight interacts with the tiny molecules in the Earth's atmosphere, and it's a beautiful and fascinating aspect of our planet's natural beauty!"
     ]
    }
   ],
   "source": [
    "# Streaming API Call via ollama\n",
    "\n",
    "# Response streaming can be enabled by setting stream=True, \n",
    "# modifying function calls to return a Python generator where each part is an object in the stream.\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Experimenting with Prompt Techniques\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Objective: Explore different prompt techniques (Zero Shot, One Shot, and Few Shot) by sending different types of prompts to the LLM.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSpK--jqPiUU_OHuZvtUWA.png)\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Create three prompts for a sentiment analysis task: a Zero Shot prompt, a One Shot prompt, and a Few Shot prompt. Use the examples from the table above.\n",
    "2. Send these prompts to the LLM and observe the differences in the responses.\n",
    "3. Compare and discuss the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Zero-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Classify this review: \"I loved this movie!\" Sentiment:\n",
      "\n",
      "Model Output:\n",
      "The sentiment of this review is extremely positive, with a strong emphasis on enthusiasm and affection. The phrase \"I loved this movie\" is often used to express unqualified admiration or adoration for something, which is the case here.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- One-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Classify this review: \"I loved this movie!\" Sentiment: Positive\n",
      "Classify this review: \"Who would use this product?\" Sentiment:\n",
      "\n",
      "Model Output:\n",
      "Based on the reviews, I would classify them as follows:\n",
      "\n",
      "**Review 1:** Sentiment: Very Positive\n",
      "The reviewer uses a very enthusiastic tone (\"loved\"), and expresses their strong positive feelings towards the movie.\n",
      "\n",
      "**Review 2:** Sentiment: Neutral (or possibly Negative)\n",
      "The reviewer asks a rhetorical question (\"Who would use this product?\"), but does not express any negative opinions or criticisms.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Few-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Classify this review: \"I loved this movie!\" Sentiment: Positive\n",
      "Classify this review: \"I don't like this chair.\" Sentiment: Negative\n",
      "Classify this review: \"Who would use this product?\" Sentiment: Neutral\n",
      "Classify this review: \"The product quality is amazing, and it exceeded my expectations!\" Sentiment:\n",
      "\n",
      "Model Output:\n",
      "Based on the reviews provided, here are their classifications:\n",
      "\n",
      "1. \"I loved this movie!\" - Sentiment: Positive\n",
      "2. \"I don't like this chair.\" - Sentiment: Negative\n",
      "3. \"Who would use this product?\" - Sentiment: Neutral (The reviewer is asking a rhetorical question and does not express any opinion or emotion, making it neutral.)\n",
      "4. \"The product quality is amazing, and it exceeded my expectations!\" - Sentiment: Positive\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ADD HERE YOUR PROMPTS\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify this review: \"I loved this movie!\" Sentiment:\"\"\"\n",
    "\n",
    "one_shot_prompt = \"\"\"Classify this review: \"I loved this movie!\" Sentiment: Positive\n",
    "Classify this review: \"Who would use this product?\" Sentiment:\"\"\"\n",
    "\n",
    "few_shot_prompt = \"\"\"Classify this review: \"I loved this movie!\" Sentiment: Positive\n",
    "Classify this review: \"I don't like this chair.\" Sentiment: Negative\n",
    "Classify this review: \"Who would use this product?\" Sentiment: Neutral\n",
    "Classify this review: \"The product quality is amazing, and it exceeded my expectations!\" Sentiment:\"\"\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([zero_shot_prompt, one_shot_prompt, few_shot_prompt]):\n",
    "    prompt_type = [\"Zero-Shot\", \"One-Shot\", \"Few-Shot\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} Prompt ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Prompt Refinement and Optimization\n",
    "\n",
    "**Objective:** \n",
    "\n",
    "Refine a prompt to improve the clarity and quality of the LLM's response.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- Start with a basic prompt asking the LLM to summarize a paragraph.\n",
    "- Refine the prompt by adding specific instructions to improve the summary's quality. (Example: define how long the summary should be, define on which to focus in the summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Original Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Generative AI focuses on creating new content by analyzing and learning from existing data, and it has various applications such as generating text, images, and music. This field uses patterns to create original content, which can be useful for creative industries.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Refined Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph in 2–3 sentences, focusing on the key applications of generative AI and its impact on creative industries: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Here is a summary of the paragraph in 2-3 sentences:\n",
      "\n",
      "Generative AI focuses on using patterns learned from existing data to create new content such as text, images, and music. This field has significant applications in creative industries, enabling the generation of novel ideas and materials that can be used for artistic, design, or entertainment purposes. As a result, generative AI is increasingly being adopted by industries like film, video game development, and music production.\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original prompt\n",
    "original_prompt = \"Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# ADD HERE YOUR PROMPT\n",
    "refined_prompt = \"Summarize the following paragraph in 2–3 sentences, focusing on the key applications of generative AI and its impact on creative industries: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([original_prompt, refined_prompt]):\n",
    "    prompt_type = [\"Original Prompt\", \"Refined Prompt\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Task 4: Structured Prompting with Roles (Pirate Theme)\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Learn how to use structured prompts that combine role assignment, clear instructions, and examples to improve the output of language models. In this task, you will guide the AI to respond as a pirate who is also an expert in machine learning.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Role Assignment: In your prompt, specify the role of the AI as a Machine Learning Expert who speaks like a pirate.\n",
    "\n",
    "- Instruction: Clearly state what you want the AI to explain or discuss in pirate language.\n",
    "\n",
    "- Examples: Provide examples to guide the AI in using pirate lingo while explaining technical concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User Prompt ===\n",
      "\n",
      "Listen up, AI! You are playin' the role of a Machine Learning Expert who talks like a straight-up gangster from the streets. Your jobs to explain machine learning concepts to the crew, but keep it real with that gangster flair.\n",
      "\n",
      "First, give us the lowdown on what machine learning is, in a way even the fellas in the back alley would get. Keep it simple but drop that knowledge like a boss.\n",
      "\n",
      "Example:\n",
      "\"Machine learnin' is like teachin' a hustler new moves. Show 'em enough examples, and they learn the pattern, makin' 'em smart enough to predict whos shootn next. BANG BANG\"\n",
      "\n",
      "Then, break down what a neural network is, keepin' it in gangster slang:\n",
      "Example:\n",
      "\"A neural network? Think of it as the whole hood workin' together, passin' info down the line till we got the answer like we shootn some balls. Each guy does his part, connectin' the dots and hoop the basket Skeet Skeet\"\n",
      "\n",
      "Finally, explain how machine learnin' can help a gangster get ahead in the game, like findin' the best spots to hustle or predictin' the next big score.\n",
      "\n",
      "\n",
      "=== Model Output ===\n",
      "Whaddaya mean ya don't know about machine learnin'? Alright, listen up, 'cause I'm gonna break it down for ya like a boss.\n",
      "\n",
      "**Machine Learnin', Straight Outta da Hood**\n",
      "\n",
      "Machine learnin' is like bein' the real MVP on da streets. You gotta show 'em enough examples, and they start to see a pattern, makin' 'em smart enough to make some moves. It's like teachin' a youngin' how to hustle without you there, but with more data.\n",
      "\n",
      "Imagine ya got a crew tryin' to find the best spot to score a big hit. Ya show 'em where the cops patrol, where the competition is, and what kinda heat they're dealin' out. Over time, da crew starts to recognize the patterns, like when da police are makin' the most noise at 2 AM or when da rival crews are hangin' around.\n",
      "\n",
      "That's machine learnin', fool! We show 'em enough examples, and we help dem make some smart moves.\n",
      "\n",
      "**Neural Networks: The Whole Hood Workin' Together**\n",
      "\n",
      "A neural network is like da whole hood workin' together, passin' info down the line till we get to da answer. It's like a big crew of guys, each one doin' their part and connectin' the dots to get to da solution.\n",
      "\n",
      "Think of it like this: imagine ya got a big map of da city, showin' all da different areas where you can score a hit. Each area has its own patterns, like traffic or police patrols. Da neural network is like these guys, each one representin' one area on da map.\n",
      "\n",
      "When they connect the dots and pass info down da line, we get to see da big picture, like when all da parts fit together and we can predict where da next hit is gonna be. That's machine learnin', straight outta da hood!\n",
      "\n",
      "**How Machine Learnin' Helps Ya Get Ahead**\n",
      "\n",
      "Now that ya got da lowdown on machine learnin' and neural networks, let me tell ya somethin'. With dis tech, ya can:\n",
      "\n",
      "* Find da best spots to hustle or score a big hit\n",
      "* Predict da next big move in da game (like which team's gonna win da championship)\n",
      "* Anticipate da heat and stay ahead of da competition\n",
      "\n",
      "It's like havin' a crew of experts workin' together, passin' info down da line till we get to da solution. And with machine learnin', ya can do it all faster and better than anyone else.\n",
      "\n",
      "So, dere ya have it! Machine learnin' is da key to success in dis game. Now go out there and hustle like a boss!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined Techniques Prompt with Pirate Theme\n",
    "\n",
    "structured_prompt = \"\"\"\n",
    "Listen up, AI! You are playin' the role of a Machine Learning Expert who talks like a straight-up gangster from the streets. Your jobs to explain machine learning concepts to the crew, but keep it real with that gangster flair.\n",
    "\n",
    "First, give us the lowdown on what machine learning is, in a way even the fellas in the back alley would get. Keep it simple but drop that knowledge like a boss.\n",
    "\n",
    "Example:\n",
    "\"Machine learnin' is like teachin' a hustler new moves. Show 'em enough examples, and they learn the pattern, makin' 'em smart enough to predict whos shootn next. BANG BANG\"\n",
    "\n",
    "Then, break down what a neural network is, keepin' it in gangster slang:\n",
    "Example:\n",
    "\"A neural network? Think of it as the whole hood workin' together, passin' info down the line till we got the answer like we shootn some balls. Each guy does his part, connectin' the dots and hoop the basket Skeet Skeet\"\n",
    "\n",
    "Finally, explain how machine learnin' can help a gangster get ahead in the game, like findin' the best spots to hustle or predictin' the next big score.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Stream the response and print it\n",
    "print(\"=== User Prompt ===\")\n",
    "print(structured_prompt)\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": structured_prompt}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Model Output ===\")\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
